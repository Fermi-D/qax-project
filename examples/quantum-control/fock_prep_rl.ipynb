{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8c16b3-76cf-4de8-89c2-c0ec0e415324",
   "metadata": {},
   "source": [
    "# Model-Free Quantum Control with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fabc20-b76a-498f-8611-d52bbe791d17",
   "metadata": {},
   "source": [
    "Author: R.Maekura (ryomaekura@g.ecc.ut-kyo.ac.jp) \\\n",
    "This notebook is based on the following paper. It provides a `QAX` implementation of the part of the Fock state preparation that is optimized using reinforcement learning. \\\n",
    "V.V.Sivak et.al., \"Model-Free Quantum Control with Reinforcement Learning\", Phys.Rev.X 12, 011059 â€“ Published 28 March, 2022 \\\n",
    "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.12.011059"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d6e9f-e9a9-443c-914b-1690f500259b",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352d0556-22bf-4881-8e76-538535018326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/users/u0001529/ondemand/qax-project\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "import qax\n",
    "from qax import state\n",
    "from qax import operator as op\n",
    "from qax.utils import linalg\n",
    "from qax.utils import device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1fbfd-6bef-419f-a633-8929af518c37",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1d1dc-eb9e-412c-83df-a9696fd3d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FockEnv:\n",
    "    def __init__(self, sys_dim: int, target_state: StateVector) -> None: \n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        self.sys_dim = sys_dim\n",
    "        self.target_state = target_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset at the beginning of an episode.\n",
    "        - Return to the vacuum state\n",
    "        - Set the step counter to 0\n",
    "        Returns:\n",
    "            observation: Observation returned to the agent\n",
    "        \"\"\"\n",
    "        # initialization\n",
    "        self._state = op.vacuum(self.sys_dim)\n",
    "        self._step_count = 0\n",
    "\n",
    "        # Return the observation\n",
    "        observation = jnp.array([self._step_count], dtype=jnp.float32)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Apply a gate for one step.\n",
    "        Args:\n",
    "            action: Action output by the agent (parameters for Displacement/SNAP gates)\n",
    "\n",
    "        - Create a Displacement gate using parameters alpha_real, alpha_imag\n",
    "        - Create a SNAP gate using the parameters phi\n",
    "\n",
    "        action format:\n",
    "        [alpha_real, alpha_imag, phi_0, phi_1, ..., phi_(N-1)]\n",
    "        Total (N + 2) dimensions\n",
    "\n",
    "        Apply the unitary operator:\n",
    "        U = D^\\dagger(alpha) * SNAP(phi_0,...,phi_{N-1}) * D(alpha)\n",
    "\n",
    "        Returns:\n",
    "            observation: Observation of the next state\n",
    "            reward: The reward\n",
    "            done: Episode termination flag\n",
    "            info: Debugging information (dict)\n",
    "        \"\"\"\n",
    "        # Create Displacement and SNAP gates from alpha, phi and apply the composite gate\n",
    "        alpha_real = action[0]\n",
    "        alpha_imag = action[1]\n",
    "        alpha = alpha_real + 1j * alpha_imag\n",
    "\n",
    "        # Create the Displacement gate\n",
    "        D_op = qt.displace(self.N, alpha)\n",
    "\n",
    "        # Create the SNAP gate: apply independent parameters for each energy level\n",
    "        phi_list = action[2:]  # Assumes shape is (N,)\n",
    "        snap_diag = np.exp(1j * phi_list)    # 1D array of shape (N,)\n",
    "        snap_mat = np.diag(snap_diag)        # Diagonal matrix of shape (N,N)\n",
    "\n",
    "        SNAP_op = qt.Qobj(snap_mat, dims=[[self.N], [self.N]])\n",
    "\n",
    "        # --- Composite gate U = D^\\dagger * SNAP * D ---\n",
    "        # Since D^\\dagger(alpha) = D(-alpha), in qutip this can be written as:\n",
    "        #   D_op.dag() or displace(N, -alpha)\n",
    "        D_dagger = D_op.dag()  # In qutip, displace(N, alpha).dag() == displace(N, -alpha)\n",
    "        U_step = D_dagger * SNAP_op * D_op\n",
    "\n",
    "        # Apply the gate to the current state\n",
    "        self._state = U_step * self._state\n",
    "\n",
    "        # Advance the step\n",
    "        self._step_count += 1\n",
    "\n",
    "        # Termination check\n",
    "        done = (self._step_count >= self.T)\n",
    "\n",
    "        if done:\n",
    "            # Calculate reward at the end of the episode: overlap (fidelity) with the Fock state\n",
    "            fidelity = qt.fidelity(self._state, self._target_state)\n",
    "            if np.random.rand() < fidelity:\n",
    "                reward = 1.0\n",
    "            else:\n",
    "                reward = -1.0\n",
    "        else:\n",
    "            # Reward is 0 while the episode continues\n",
    "            reward = 0\n",
    "\n",
    "        # Return the observation\n",
    "        observation = np.array([self._step_count], dtype=np.float32)  # Return the step count as the observation\n",
    "\n",
    "        # info returns debugging information\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, done, info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bceab05-8960-4568-9098-18f0bbb6e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nnx.Module):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim (int): Dimension of the observation.\n",
    "            action_dim (int): Dimension of the action.\n",
    "            hidden_dim (int): Dimension of the hidden layers and LSTM state.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # --- Network Layers ---\n",
    "        # LSTM layer to process sequences of observations\n",
    "        self.lstm = nn.LSTM(obs_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Fully connected layers to process the LSTM's output\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean_layer = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        # Learn a single log_std parameter for each action dimension\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
